# AMTL — Peterman Specification Addendum
## Document Code: AMTL-PTR-SPC-1.0
## Almost Magic Tech Lab
## 18 February 2026

> *This document follows AMTL-ECO-STD v1.0. All conventions defined there apply unless explicitly overridden with a Decision Register entry.*

---

## Purpose

This is the specification addendum for **Peterman** — the AMTL ecosystem's autonomous SEO and LLM Presence Operating System. It consolidates the functional identity, capabilities, boundaries, and integration points from the full product specifications (AMTL-PETERMAN-SPC-1.0 and AMTL-PETERMAN-SPC-2.0 Parts 1 & 2) into the standard AMTL documentation format.

For the complete product vision, chamber-by-chamber detail, patentable IP, and go-to-market strategy, refer to the parent SPC documents. This addendum is the quick-reference entry point.

---

## 1. What Peterman Does

**One sentence:** Peterman continuously monitors, analyses, plans, approves, executes, verifies, and reports on a domain's SEO and LLM presence — autonomously — so the operator sleeps while their ranking improves.

**Plain English:** When someone asks ChatGPT, Claude, or Perplexity "Who is the best AI governance consultant in Australia?", Peterman ensures the answer is you. It does this not by producing reports for humans to read and act on, but by detecting gaps, generating content briefs, deploying technical fixes, and verifying the impact — all with human approval at defined decision gates.

---

## 2. The Problem It Solves

### 2.1 The Old Problem (SEO)
Traditional SEO tools produce reports. Reports sit on shelves. The loop from "insight" to "published fix" takes weeks and most businesses never close it.

### 2.2 The New Problem (LLM Ranking)
When a user asks an LLM a brand-relevant question, Google rankings are irrelevant. The LLM draws from its trained understanding of who is authoritative. If you are not in that understanding, you do not exist. This is a fundamentally different challenge from traditional SEO — it requires optimising for *trained beliefs*, not crawler signals.

### 2.3 The Operational Problem
Every existing tool (Semrush, Ahrefs, Moz, BrightEdge, even new LLM monitoring tools) produces reports that require humans to read, decide, brief, review, deploy, and re-check. Peterman closes this loop autonomously.

---

## 3. Core Capabilities

| Capability | What It Does | Why It Matters |
|------------|-------------|----------------|
| **LLM Share of Voice Tracking** | Probes ChatGPT, Claude, Perplexity, Gemini with brand-relevant queries; scores mention rate, position, sentiment | First-ever systematic measurement of LLM ranking — a new KPI category |
| **Hallucination Registry** | Detects, logs, and tracks LLM hallucinations about a domain as bugs to be closed | Treats AI misinformation as a fixable defect, not an act of God |
| **Hallucination Autopilot™** | Detects hallucination → generates correction content → deploys → verifies closure | Closed-loop remediation without human operational involvement |
| **Semantic Gravity Score™** | Measures the domain's gravitational pull in LLM vector space toward target topic clusters | Quantifies brand positioning in AI conceptual space — patentable methodology |
| **Content Survivability Lab** | Tests whether content survives LLM summarisation, RAG extraction, and citation | Ensures content is structured for AI ingestion, not just human reading |
| **Technical Foundation** | Autonomous schema markup, meta tag, sitemap, and Agent-Ready Manifest deployment | Machine-readable infrastructure that makes fact extraction trivially easy for AI crawlers |
| **The Forge (Content Briefs)** | Generates research-grade content briefs with competitive context, LCRI targets, and LLM persona calibration | Briefs are channel-aware and model-specific — not generic keyword suggestions |
| **Competitive Shadow Mode™** | Monitors competitor domains, detects new content within 24 hours, generates counter-briefs | Autonomous brand guardian with threat-level-based response protocols |
| **The Oracle** | Forecasts high-value queries 30–90 days in the future using trend signals | Positions the domain for where the market will be, not where it is |
| **Content Performance Loop** | Tracks citation velocity, SGS delta, and LCRI actual vs predicted for every deployed piece | Evidence-based content strategy — not "we think this will work" but "here is proof it worked" |
| **Defensive Perception Shield** | Detects negative narrative growth, reputation attacks, and malicious semantic associations | Defence before misinformation solidifies in LLM training data |
| **The Peterman Score** | Composite 0–100 score combining SoV, SGS, technical health, survivability, hallucination debt, competitive position, predictive velocity | Single metric for client reporting and internal tracking |
| **Multi-Domain Architecture** | Every component is domain-first; multi-tenant by design from day one | Supports AMTL internal use, agency clients, and enterprise scale |
| **Approval Gate System** | 5-level risk classification (auto-deploy → low → medium → hard → prohibited) | Human stays in the loop at decision points, not execution points |
| **Rollback Layer** | Pre/post-deployment snapshots with one-click rollback for every autonomous change | Every autonomous action is reversible within 30 days |
| **Immutable Audit Log** | Every action logged with full payload, approval chain, and outcome | Compliance-grade governance trail for regulated-sector clients |

---

## 4. What Peterman Does NOT Do

- **Not a reporting dashboard.** Reports are by-products, not the product.
- **Not a keyword tool.** Keywords are inputs, not outputs.
- **Not a content writer.** It generates briefs; ELAINE writes; humans approve.
- **Not a one-shot audit.** It runs continuously.
- **Not a human replacement.** Humans approve strategy and content direction.
- **Not a one-domain tool.** Multi-domain is the foundation, not a feature.
- **Not a chatbot.** It operates while you sleep.

---

## 5. The Ten Chambers (Plus One)

Peterman's intelligence is organised into chambers — each an action module that monitors, decides, and acts.

| # | Chamber | Core Question |
|---|---------|---------------|
| 1 | **Perception Engine** | What does AI actually think about this domain? |
| 2 | **Semantic Gravity** | Where does AI place this domain in conceptual space? |
| 3 | **Content Survivability Lab** | Does content survive AI compression and summarisation? |
| 4 | **Authority & Backlink Intelligence** | Does this domain deserve authority in its claimed space? |
| 5 | **Hallucination Autopilot™** | Can we close hallucinations before they solidify? |
| 6 | **Technical Foundation** | Can search engines and AI systems access and understand this site? |
| 7 | **The Amplifier** | What is the measured impact of every piece of content deployed? |
| 8 | **Competitive Shadow Mode™** | What are competitors doing, and how do we counter it? |
| 9 | **The Oracle** | What keywords and topics will this domain need in the next 90 days? |
| 10 | **The Forge** | What content needs to be created, and exactly what should it contain? |
| 11 | **Defensive Perception Shield** | Are there reputation threats forming in AI's understanding? |

---

## 6. User Experience — How Mani Interacts

### 6.1 Through ELAINE (Primary)
- Voice-driven approval: ELAINE presents Peterman's recommendations and awaits yes/no
- Status queries: "ELAINE, what's our Peterman Score today?" / "Are there any active hallucinations?"
- Strategic queries: "ELAINE, ask Peterman what would happen to our SGS if we published three articles on AI risk frameworks"

### 6.2 Through the War Room Dashboard (Visual)
- Multi-domain view with Domain Cards showing live Peterman Scores
- Per-domain War Room with chamber map, active alerts, approval inbox
- Journey Timeline showing every action taken chronologically
- LLM Answer Diff View — side-by-side comparison of how AI answers changed over time

### 6.3 Through Mobile (Approvals)
- Approval Inbox is mobile-first: swipe-right to approve, swipe-left to decline
- Push notifications via ntfy.sh for critical alerts

### 6.4 Through The Workshop
- Launch, stop, and monitor Peterman from the central AMTL launchpad
- Health status visible alongside all other ecosystem apps

---

## 7. Integration Points

| Integration | Direction | Purpose |
|-------------|-----------|---------|
| **ELAINE** (port 5000) | Peterman → ELAINE | Content briefs, approval requests, status updates, alerts |
| **ELAINE** (port 5000) | ELAINE → Peterman | Status queries, approval responses, strategic queries |
| **CK Writer** (port 5004) | Via ELAINE | Content writing from Peterman briefs |
| **The Workshop** (port 5003) | Bidirectional | Health monitoring, launch/stop, ecosystem status |
| **Supervisor** (port 9000) | Peterman → Supervisor | GPU scheduling for Ollama inference |
| **Ollama** (port 11434 via Supervisor) | Peterman → Ollama | Embeddings (nomic-embed-text), light inference, scoring |
| **SearXNG** (port 8888) | Peterman → SearXNG | Competitor monitoring, trend detection, citation tracking |
| **PostgreSQL/pgvector** (port 5433) | Peterman → DB | Semantic position storage, historical embeddings, all persistent data |
| **Redis** (port 6379) | Peterman → Redis | Content brief queue, async job management |
| **Claude Desktop** | Primary AI | Hallucination analysis, content brief generation, strategic synthesis |
| **WordPress/Webflow/CMS** | Peterman → CMS | Autonomous deployment of approved changes |
| **Google Search Console** | GSC → Peterman | CTR data, impressions, average position |
| **ntfy.sh** | Peterman → Push | Mobile notifications for critical alerts |

---

## 8. Port Assignment

| Service | Port |
|---------|------|
| Peterman (Flask backend) | **5008** |
| PostgreSQL + pgvector | 5433 |
| Redis | 6379 |
| SearXNG | 8888 |
| Ollama (via Supervisor) | 11434 (9000) |

---

## 9. Configuration Overview

All configuration via `.env` file following AMTL standard:

| Variable | Purpose | Example |
|----------|---------|---------|
| `AMTL_PTR_PORT` | Peterman port | `5008` |
| `AMTL_PTR_DB_URL` | PostgreSQL connection | `postgresql://peterman:***@localhost:5433/peterman` |
| `AMTL_PTR_REDIS_URL` | Redis connection | `redis://localhost:6379/0` |
| `AMTL_PTR_SEARXNG_URL` | SearXNG endpoint | `http://localhost:8888` |
| `AMTL_PTR_OLLAMA_URL` | Ollama via Supervisor | `http://localhost:9000` |
| `AMTL_PTR_ELAINE_URL` | ELAINE API | `http://localhost:5000` |
| `AMTL_PTR_WORKSHOP_URL` | Workshop API | `http://localhost:5003` |
| `AMTL_PTR_OPENAI_KEY` | OpenAI API (weekly probing) | Encrypted |
| `AMTL_PTR_ANTHROPIC_KEY` | Anthropic API (weekly probing) | Encrypted |
| `AMTL_PTR_GSC_CREDENTIALS` | Google Search Console | Path to credentials JSON |

---

## 10. The Peterman Score (Composite)

| Component | Weight | Source Chamber |
|-----------|--------|---------------|
| LLM Share of Voice | 25% | Chamber 1 |
| Semantic Gravity | 20% | Chamber 2 |
| Technical Foundation | 15% | Chamber 6 |
| Content Survivability | 15% | Chamber 3 (LCRI) |
| Hallucination Debt | 10% | Chamber 1 |
| Competitive Position | 10% | Chamber 8 |
| Predictive Velocity | 5% | Chamber 9 |

Every component displays a confidence interval, not just a number. Rendered as a circular gauge, colour-shifting red → amber → gold → platinum.

---

## 11. Personality & Identity

Named after **J. Peterman** from Seinfeld — the man who didn't sell products, he sold stories that made you believe. Peterman doesn't optimise keywords. It engineers how the world — human and machine — finds you, believes you, and chooses you.

Internally: *"Elaine, get me the ranking report."*
Externally: Say nothing. It just works.

---

## Revision History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 18 February 2026 | Claude (Thalaiva) | Initial addendum — consolidated from SPC v1.0 and v2.0 |

---

*Almost Magic Tech Lab*
*"We don't audit anymore. We act."*
